# Name Generation with Bigram Models and Neural Networks

This notebook explores techniques for generating new names based on a dataset of existing names. It covers two main approaches:

1.  **Bigram Language Model**: A statistical model that predicts the next character based on the current character.
2.  **Neural Network Language Model**: A simple neural network (single layer) that learns to predict the next character.

## Table of Contents

- [Data Loading and Initial Exploration](#data-loading-and-initial-exploration)
- [Bigram Language Model](#bigram-language-model)
- [Neural Network Language Model](#neural-network-language-model)
- [Results and Generation](#results-and-generation)

## Data Loading and Initial Exploration

The notebook starts by loading a list of words from `name.txt`. Basic statistics about the dataset are then calculated.

- **Number of words**: 32033
- **Shortest word length**: 2
- **Longest word length**: 15

Example words:

['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia', 'harper', 'evelyn']

## Bigram Language Model

This section constructs a bigram language model to understand the probabilities of character sequences. Special tokens `<S>` (start) and `<E>` (end) are used to mark word boundaries. The model counts the occurrences of each bigram (e.g., 'em', 'ma') and converts these counts into probabilities.

- **Count Matrix (N)**: A 27x27 matrix (26 alphabet characters + '.' for start/end) storing bigram counts.
- **Probability Matrix (P)**: Derived from the count matrix, representing the probability of the second character given the first.
- **Model Smoothing**: `N+1` is used to avoid zero probabilities and infinite negative log-likelihoods.

Visualization of the bigram count matrix:

<img src="https://raw.githubusercontent.com/yvprat/Make-More-Like-This-Name-Generator-/main/assets/N_Matrix.png" alt="Bigram Count Matrix" width="500"/>

_Note: The image is a placeholder and should be replaced with the actual generated plot._

**Loss Function**: Negative Log-Likelihood (NLL) is used to evaluate the model, with lower values indicating a better fit. The average NLL for the bigram model is approximately `2.454`.

## Neural Network Language Model

This section implements a simple neural network to learn bigram probabilities. Each character is represented by a one-hot encoding.

- **Input**: One-hot encoded character (27 dimensions).
- **Weights (W)**: A 27x27 matrix, where each row represents the weights for predicting the next character given the input character.
- **Logits**: `xenc @ W` (matrix multiplication of input with weights).
- **Counts**: `logits.exp()` (exponentiating logits to get 'pseudo-counts').
- **Probabilities**: `counts / counts.sum(1, keepdims=True)` (softmax activation to get probabilities).
- **Loss Function**: Average Negative Log-Likelihood of the correct next characters.

**Training**: The network is trained using gradient descent to minimize the NLL. The learning rate is set to `50`.

After 100 training steps, the average NLL for the neural network model is approximately `2.456`.

## Results and Generation

Both models can generate new names by sampling characters sequentially based on the learned probabilities. The neural network, despite its simplicity, achieves a comparable NLL to the statistical bigram model.

Example names generated by the neural network:

mmyaxomarynnailomev. zlyailvilinnantie. ceenthi. ylee. vineee.

This notebook demonstrates a fundamental approach to sequence generation, laying the groundwork for more complex models like recurrent neural networks.

# Character-Level Bigram Language Model

## Project Overview

This project implements and compares two distinct approaches to building a character-level language model for name generation. The goal is to train a model on a dataset of names to generate novel, statistically similar names.

The notebook explores two equivalent methods to solve this problem:

1.  **Count-Based Statistical Model**: Directly calculating transition probabilities from bigram counts.
2.  **Neural Network Model**: A single-layer neural network trained via gradient descent to minimize the Negative Log Likelihood (NLL).

## Dataset & Preprocessing

- **Source**: `name.txt` containing 32,033 unique names.
- **Vocabulary**: 27 characters
  - 26 lowercase alphabet letters (`a-z`).
  - 1 special character `.` used as a start/end token (Index 0).
- **Mappings**:
  - `stoi`: String-to-Index mapping (e.g., 'a' → 1, '.' → 0).
  - `itos`: Index-to-String mapping for decoding indices back to characters.
- **Data Structure**: The dataset is processed into **bigrams** (pairs of consecutive characters). For example, the name "emma" yields the training examples: `(. → e)`, `(e → m)`, `(m → m)`, `(m → a)`, `(a → .)`.

## Approach 1: Statistical Bigram Model

This approach explicitly counts the frequency of every character transition to build a probability distribution.

### Methodology

1.  **Count Matrix ($N$)**: A 27x27 matrix is created where $N_{i,j}$ represents the number of times character $j$ follows character $i$.
2.  **Model Smoothing**: A small count (1) is added to the matrix ($N+1$) to prevent zero probabilities. This ensures that the loss function does not diverge (return infinity) if the model encounters a bigram that was never seen in the training data.
3.  **Probability Matrix ($P$)**: Counts are normalized across rows to create a valid probability distribution: $P_{i,j} = \frac{N_{i,j}}{\sum_k N_{i,k}}$.

### Evaluation

- **Metric**: Average Negative Log Likelihood (NLL).
- **Formula**: $NLL = - \frac{1}{n} \sum_{i=1}^{n} \log(P(\text{target}_i | \text{input}_i))$.
- **Benchmark**: The statistical model achieves a loss of approximately **2.454**, serving as the optimal lower bound for the neural network.

## Approach 2: Neural Network Model

This approach casts the bigram modeling as a classification task. Given a character, the network predicts the probability distribution of the next character.

### Architecture

- **Input**: One-hot encoded vector of size 27 representing the current character.
- **Weights ($W$)**: A 27x27 weight matrix initialized randomly. This single layer acts as a linear transformation.
- **Forward Pass**:
  1.  **Logits**: $X_{enc} @ W$ (Matrix multiplication).
  2.  **Counts**: $\exp(\text{logits})$ (Exponentiation ensures positive values, interpreting logits as log-counts).
  3.  **Probabilities**: $\text{counts} / \sum(\text{counts})$ (Softmax activation function).
- **Loss Function**: Cross-Entropy Loss (implemented manually as NLL on the predicted probabilities).

### Optimization

- **Algorithm**: Stochastic Gradient Descent (SGD).
- **Training Loop**:
  1.  **Forward Pass**: Compute probabilities and loss.
  2.  **Backward Pass**: Compute gradients of the loss with respect to weights ($W.grad$).
  3.  **Update**: Adjust weights in the opposite direction of the gradient: $W_{data} += -\text{learning\_rate} * W_{grad}$.
- **Hyperparameters**: A learning rate of **50** is used.
- **Convergence**: Over 100 iterations, the neural network's loss converges towards the statistical model's loss (~2.457), demonstrating that the gradient-based approach learns the same underlying distribution as the count-based approach.

## Generation

Both models generate text by sampling from the learned probability distributions. Starting with the start token `.`, a character is sampled based on the current probabilities, fed back as input for the next step, and the process repeats until the end token `.` is generated.

### Example Output

mmyaxomarynnailomev. zlyailvilinnantie. ceenthi. ylee.

## Requirements

- Python 3.x
- PyTorch
- Matplotlib (for visualizing the count matrix)
